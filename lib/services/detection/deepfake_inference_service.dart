import 'dart:async';
import 'dart:io';
import 'dart:typed_data';

import 'package:flutter/foundation.dart';
import 'package:flutter/services.dart';
import 'package:path_provider/path_provider.dart';
import 'package:flutter_webrtc/flutter_webrtc.dart';

import 'frame_extractor.dart';

// --- DTOs ---

class DetectionStatus {
  static const safe = const DetectionStatus._('safe');
  static const warning = const DetectionStatus._('warning');
  static const danger = const DetectionStatus._('danger');

  final String name;
  const DetectionStatus._(this.name);

  @override
  String toString() => name;
}

class DeepfakeState {
  final double rawScore;
  final double confidence; // Moving Average
  final DetectionStatus status;

  DeepfakeState({
    required this.rawScore,
    required this.confidence,
    required this.status,
  });

  @override
  String toString() =>
      'State(conf: ${(confidence * 100).toStringAsFixed(1)}%, status: ${status.name})';
}

// --- Service ---

class DeepfakeInferenceService {
  static const MethodChannel _channel = MethodChannel(
    'com.garim.eye/inference',
  );

  final FrameExtractor _frameExtractor = FrameExtractor();
  StreamSubscription? _frameSub;

  // Intelligent Engine State
  final _stateController = StreamController<DeepfakeState>.broadcast();
  Stream<DeepfakeState> get stateStream => _stateController.stream;

  // EMA State
  double _emaScore = 0.0;
  double _previousRawScore = 0.0;
  bool _isFirstRun = true;

  // Frame sampling
  int _frameCounter = 0;

  // Logging
  IOSink? _logSink;

  // Pipeline Stats
  int _pipelineFpsCounter = 0;
  DateTime _lastFpsTime = DateTime.now();

  Future<void> initialize() async {
    debugPrint("[InferenceService] Initialized (Full-Native Pipeline).");
  }

  Future<void> start(MediaStreamTrack track) async {
    if (_frameSub != null) return;

    _isFirstRun = true;
    _previousRawScore = 0.0;
    _frameCounter = 0; // Reset counter

    await _initLogger();
    await _frameExtractor.startExtraction(track);

    // Process all frames for maximum accuracy
    // Expected: 20 frames in ~3 seconds at 16.8 FPS
    _frameSub = _frameExtractor.frameStream.listen((bytes) {
      _frameCounter++;
      unawaited(_handleFrame(bytes));
    });

    debugPrint("[InferenceService] Pipeline Started (all frames processed).");
  }

  Future<void> _handleFrame(Uint8List bytes) async {
    try {
      debugPrint(
        "ðŸ“¸ [InferenceService] Calling processFrame with ${bytes.length} bytes",
      );

      // 1. Send to Native Pipeline (Zero Logic in Dart)
      //    We send raw bytes. Native does Detection -> Crop -> Buffer -> Inference.
      //    This is async but we don't await to allow concurrent processing
      final result = await _channel.invokeMethod('processFrame', bytes);

      debugPrint("ðŸ“¦ [InferenceService] Received result: $result");

      if (result != null && result is Map) {
        _handleNativeResult(result);
      } else {
        debugPrint(
          "âš ï¸ [InferenceService] Result is null or not a Map: $result",
        );
      }
    } catch (e, stackTrace) {
      debugPrint("âŒ [InferenceService] Pipeline Error: $e");
      debugPrint("Stack trace: $stackTrace");
    }
  }

  void _handleNativeResult(Map result) {
    debugPrint("ðŸ”„ [InferenceService] Processing native result: $result");
    final status = result['status'] as String;
    debugPrint("ðŸ“Š [InferenceService] Status: $status");

    // Update Pipeline FPS
    _pipelineFpsCounter++;
    final now = DateTime.now();
    if (now.difference(_lastFpsTime).inSeconds >= 1) {
      final fps = _pipelineFpsCounter;
      // Log FPS
      _logSink?.writeln("${now.toIso8601String()},PIPELINE_FPS,,,,,$fps");
      _pipelineFpsCounter = 0;
      _lastFpsTime = now;
    }

    final detectMs = (result['detection_ms'] as num?)?.toDouble() ?? 0.0;
    final cropMs = (result['cropping_ms'] as num?)?.toDouble() ?? 0.0;

    if (status == "skipped") {
      debugPrint("â­ï¸ [InferenceService] Frame skipped: ${result['reason']}");
      // Log skip?
      // _logSink?.writeln("... SKIPPED (${result['reason']}) ...");
      return;
    }

    if (status == "inference") {
      final score = (result['score'] as num).toDouble();
      final inferMs = (result['inference_ms'] as num).toDouble();

      // Intelligent Engine Logic
      _processIntelligentScore(score, detectMs, cropMs, inferMs);
    }
  }

  // --- Intelligent Reliability Engine ---

  void _processIntelligentScore(
    double rawCnnScore,
    double detectMs,
    double cropMs,
    double inferMs,
  ) {
    // 1. Fast-Trigger
    bool fastTrigger = false;
    if (rawCnnScore >= 0.95 && _previousRawScore >= 0.95) {
      fastTrigger = true;
    }
    _previousRawScore = rawCnnScore;

    // 2. Hybrid Fusion (Placeholder)
    double fftScore = 0.0;
    double beta = 1.0;
    double hybridScore = (beta * rawCnnScore) + ((1 - beta) * fftScore);

    // 3. EMA
    const double alpha = 0.33;

    if (_isFirstRun) {
      _emaScore = hybridScore;
      _isFirstRun = false;
    } else {
      if (fastTrigger) {
        _emaScore = 0.95;
      } else {
        _emaScore = (alpha * hybridScore) + ((1 - alpha) * _emaScore);
      }
    }

    // 4. Decision
    DetectionStatus status;
    String triggerType = "NORMAL";

    if (fastTrigger) {
      status = DetectionStatus.danger;
      triggerType = "FAST_TRIGGER";
    } else {
      if (_emaScore >= 0.7) {
        status = DetectionStatus.danger;
      } else if (_emaScore >= 0.4) {
        status = DetectionStatus.warning;
      } else {
        status = DetectionStatus.safe;
      }
    }

    // 5. Emit
    final state = DeepfakeState(
      rawScore: rawCnnScore,
      confidence: _emaScore,
      status: status,
    );
    _stateController.add(state);

    // 6. Log
    _logInference(state, fftScore, triggerType, detectMs, cropMs, inferMs);

    debugPrint(
      "ðŸŽ¯ [Engine] $triggerType | Raw: ${rawCnnScore.toStringAsFixed(3)} | EMA: ${_emaScore.toStringAsFixed(3)} | Latency: ${detectMs + cropMs + inferMs}ms",
    );
  }

  // --- Logging & Utilities ---

  Future<void> _initLogger() async {
    try {
      final docDir = await getApplicationDocumentsDirectory();
      final logDir = Directory('${docDir.path}/logs');
      if (!await logDir.exists()) await logDir.create(recursive: true);

      final timestamp = DateTime.now().toIso8601String().replaceAll(':', '-');
      final logFile = File('${logDir.path}/log_native_$timestamp.txt');

      _logSink = logFile.openWrite();
      _logSink?.writeln(
        'Timestamp,Raw_CNN,Raw_FFT,Fused,EMA,Status,Trigger_Type,Detect_ms,Crop_ms,Inference_ms,Pipeline_FPS',
      );
    } catch (e) {
      debugPrint("[InferenceService] Failed to init logger: $e");
    }
  }

  void _logInference(
    DeepfakeState state,
    double fft,
    String trigger,
    double detectMs,
    double cropMs,
    double inferMs,
  ) {
    if (_logSink != null) {
      final time = DateTime.now().toIso8601String();
      _logSink?.writeln(
        '$time,${state.rawScore.toStringAsFixed(4)},${fft.toStringAsFixed(4)},'
        '${state.rawScore.toStringAsFixed(4)},${state.confidence.toStringAsFixed(4)},'
        '${state.status.name},$trigger,${detectMs.toStringAsFixed(2)},${cropMs.toStringAsFixed(2)},${inferMs.toStringAsFixed(2)},',
      );
    }
  }

  Future<void> stop() async {
    await _frameSub?.cancel();
    _frameSub = null;
    _frameExtractor.stopExtraction();

    // Close log file safely
    if (_logSink != null) {
      try {
        await _logSink!.flush();
        await _logSink!.close();
      } catch (e) {
        // Ignore flush errors during shutdown
        debugPrint("[InferenceService] Log flush error (safe to ignore): $e");
      }
      _logSink = null;
    }

    debugPrint("[InferenceService] Pipeline Stopped.");
  }

  void dispose() {
    stop();
    _stateController.close();
  }
}
